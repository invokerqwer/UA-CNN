{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from utils import setup_seed,heteroscedastic_loss,extract_coords_to_csv,random_mini_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UA_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UA_CNN, self).__init__()#N*2*4096\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=1,out_channels=2,kernel_size=3,padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool1d(2,2)#N*4*2048\n",
    "        self.conv2 = nn.Conv1d(in_channels=2,out_channels=4,kernel_size=3,padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool1d(2,2)#N*4*1024\n",
    "        self.conv3 = nn.Conv1d(in_channels=4,out_channels=2,kernel_size=3,padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.maxpool3 = nn.MaxPool1d(2,2)#N*2*512\n",
    "        self.conv4 = nn.Conv1d(in_channels=2,out_channels=1,kernel_size=3,padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.maxpool4 = nn.MaxPool1d(2,2)#N*1*256\n",
    "#        self.dropout_layer = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(256,64)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64,32)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(32,16)\n",
    "        self.relu7 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(16,8)\n",
    "        self.relu8 = nn.ReLU()\n",
    "        self.fc5 = nn.Linear(8,1)\n",
    "        \n",
    "        self.fc1_var = nn.Linear(256,64)\n",
    "        self.relu5_var = nn.ReLU()\n",
    "        self.fc2_var = nn.Linear(64,32)\n",
    "        self.relu6_var = nn.ReLU()\n",
    "        self.fc3_var = nn.Linear(32,16)\n",
    "        self.relu7_var = nn.ReLU()\n",
    "        self.fc4_var = nn.Linear(16,8)\n",
    "        self.relu8_var = nn.ReLU()\n",
    "        self.fc5_var = nn.Linear(8,1)\n",
    "         \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.maxpool1(self.relu1(self.conv1(x)))\n",
    "        out = self.maxpool2(self.relu2(self.conv2(out)))\n",
    "        out = self.maxpool3(self.relu3(self.conv3(out)))\n",
    "        out_vector = self.maxpool4(self.relu4(self.conv4(out)))\n",
    "#        out = self.dropout_layer(out)\n",
    "        out = self.fc1(out_vector)\n",
    "        out = self.relu5(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu6(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu7(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu8(out)\n",
    "        out = self.fc5(out)\n",
    "        \n",
    "        out_var = self.fc1_var(out_vector)\n",
    "        out_var = self.relu5_var(out_var)\n",
    "        out_var = self.fc2_var(out_var)\n",
    "        out_var = self.relu6_var(out_var)\n",
    "        out_var = self.fc3_var(out_var)\n",
    "        out_var = self.relu7_var(out_var)\n",
    "        out_var = self.fc4_var(out_var)\n",
    "        out_var = self.relu8_var(out_var)\n",
    "        out_var = self.fc5_var(out_var)\n",
    "        \n",
    "        return out, out_vector, out_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "parent_dir = './'\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_dir = os.path.join(parent_dir, f'results/uncertainty/{timestamp}')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "log_file_path = os.path.join(results_dir, f'training_log_{timestamp}.txt')\n",
    "# Function to log and print messages\n",
    "def log_and_print(message, log_file_path):\n",
    "    print(message)\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(message + '\\n')\n",
    "\n",
    "physical_para = pd.read_csv('./data/934label-16_no-header.csv', header=None, low_memory=False)\n",
    "XRD_descriptor = pd.read_csv('./data/XRD_descriptor_936.csv', header=None, low_memory=False)\n",
    "data = pd.concat([XRD_descriptor, physical_para.iloc[:,0]], axis=1)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "heteroscedastic_loss_coefficient = 1e-3\n",
    "grid_points = [[12,52]]\n",
    "results_all = []\n",
    "LR = 0.001\n",
    "epochs = 1000\n",
    "mb_size = 50\n",
    "r2_record = -math.inf\n",
    "seeds = [1992, 30, 27, 35, 81]\n",
    "seeds_len = 5\n",
    "indices = data.index\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=0)\n",
    "x_train = data.iloc[train_indices, :4096]\n",
    "x_test = data.iloc[test_indices, :4096]\n",
    "y_train = data.iloc[train_indices, 4096]\n",
    "y_test = data.iloc[test_indices, 4096]\n",
    "\n",
    "y_test_ = torch.tensor(y_test.values) \n",
    "idx_test = torch.nonzero(y_test_.squeeze() != 0, as_tuple=False)\n",
    "x_test_len = len(idx_test)\n",
    "log_and_print(f\"x_test_len: {x_test_len}\", log_file_path)\n",
    "\n",
    "x_train = torch.from_numpy(x_train.values).float().to(device)\n",
    "x_test = torch.from_numpy(x_test.values).float().to(device)\n",
    "y_train = torch.from_numpy(y_train.values).float().to(device)\n",
    "y_test = torch.from_numpy(y_test.values).float().to(device)\n",
    "\n",
    "x_train = torch.unsqueeze(x_train, 1)\n",
    "x_test = torch.unsqueeze(x_test, 1)\n",
    "\n",
    "sum_preds = np.zeros((x_test_len, 1))\n",
    "sum_ale_uncs = np.zeros((x_test_len, 1))\n",
    "sum_epi_uncs = np.zeros((x_test_len, 1))\n",
    "all_preds = np.zeros((x_test_len, 1, seeds_len))\n",
    "\n",
    "for grid_point in tqdm(grid_points):\n",
    "    results=[]\n",
    "    result_r2_test = pd.DataFrame(np.zeros([10,11]),columns=['seed','F1','F2','F3','F4','F5','F6','F7','F8','F9','F10'])\n",
    "    result_mae_test = pd.DataFrame(np.zeros([10,11]),columns=['seed','F1','F2','F3','F4','F5','F6','F7','F8','F9','F10'])\n",
    "    result_loss_test = pd.DataFrame(np.zeros([10,11]),columns=['seed','F1','F2','F3','F4','F5','F6','F7','F8','F9','F10'])\n",
    "    result_r2_train = pd.DataFrame(np.zeros([10,11]),columns=['seed','F1','F2','F3','F4','F5','F6','F7','F8','F9','F10'])\n",
    "    result_mae_train = pd.DataFrame(np.zeros([10,11]),columns=['seed','F1','F2','F3','F4','F5','F6','F7','F8','F9','F10'])\n",
    "    result_loss_train = pd.DataFrame(np.zeros([10,11]),columns=['seed','F1','F2','F3','F4','F5','F6','F7','F8','F9','F10'])\n",
    "    for i_seed in tqdm(range(seeds_len), position=0, leave=True): # Use seeds_len to match the initialization\n",
    "        seed = seeds[i_seed] \n",
    "        result_r2_train.iloc[i_seed,0] = seed\n",
    "        result_mae_train.iloc[i_seed,0] = seed\n",
    "        result_loss_train.iloc[i_seed,0] = seed\n",
    "        result_r2_test.iloc[i_seed,0] = seed\n",
    "        result_mae_test.iloc[i_seed,0] = seed\n",
    "        result_loss_test.iloc[i_seed,0] = seed\n",
    "        \n",
    "        setup_seed(seed)\n",
    "\n",
    "        i = 0    \n",
    "     \n",
    "        input_size, feature_size = x_train.shape[0], x_train.shape[1]\n",
    "  \n",
    "        loss_train_log = []\n",
    "        loss_test_log = []\n",
    "        \n",
    "        model = UA_CNN().to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "        loss_func = nn.MSELoss()        \n",
    "                \n",
    "        y_train_sub = y_train\n",
    "        y_test_sub = y_test\n",
    "        r2_best = -math.inf\n",
    "        MAE_best = 0\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            num_minibatches = int(input_size / mb_size) + 1\n",
    "            minibatches = random_mini_batches(x_train, y_train_sub, mb_size)\n",
    "            model.train()\n",
    "            for minibatch in minibatches:\n",
    "                batch_x, batch_y  = minibatch\n",
    "                batch_y_pre, _, batch_y_pre_log_var = model(batch_x)\n",
    "                idx = torch.nonzero(batch_y.squeeze()!=0,as_tuple=False)\n",
    "                batch_y_pre1 = torch.index_select(batch_y_pre.squeeze(), dim=0, index = idx.squeeze())\n",
    "                batch_y_pre_log_var1 = torch.index_select(batch_y_pre_log_var.squeeze(), dim=0, index = idx.squeeze())\n",
    "                batch_y1 = torch.index_select(batch_y.squeeze(), dim=0, index = idx.squeeze())\n",
    "                mse_loss = loss_func(batch_y_pre1.squeeze(), batch_y1.squeeze())    \n",
    "                h_loss = heteroscedastic_loss(batch_y1.squeeze(),batch_y_pre1.squeeze(),batch_y_pre_log_var1.squeeze())\n",
    "                loss = mse_loss + heteroscedastic_loss_coefficient * h_loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss = epoch_loss + (loss / num_minibatches)\n",
    "            loss_train_log.append(torch.mean(epoch_loss).item())\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_test_pre, _, y_test_pre_log_var = model(x_test)\n",
    "                idx_test = torch.nonzero(y_test_sub.squeeze()!=0,as_tuple=False)\n",
    "                y_test_pre1 = torch.index_select(y_test_pre.squeeze(), dim=0, index = idx_test.squeeze())\n",
    "                y_test_pre_log_var1 = torch.index_select(y_test_pre_log_var.squeeze(), dim=0, index = idx_test.squeeze())\n",
    "                y_test_sub1 = torch.index_select( y_test_sub.squeeze(), dim=0, index = idx_test.squeeze())        \n",
    "               \n",
    "                h_loss_test = heteroscedastic_loss(y_test_sub1.squeeze(),y_test_pre1.squeeze(),y_test_pre_log_var1.squeeze())\n",
    "                mse_loss_test = loss_func(y_test_pre1.squeeze(), y_test_sub1.squeeze())\n",
    "                loss_test = mse_loss_test + heteroscedastic_loss_coefficient * h_loss_test\n",
    "                MAE2 = mean_absolute_error(y_test_sub1.cpu().numpy().squeeze(),y_test_pre1.cpu().numpy().squeeze())\n",
    "                r2_score_v =  r2_score(y_test_sub1.cpu().numpy().squeeze(),y_test_pre1.cpu().numpy().squeeze()) \n",
    "                \n",
    "                if epoch % 20 == 0:\n",
    "                    log_message = f'Iter-{epoch}; Total loss: {loss_test.item():.4f}; MAE2: {MAE2:.4f}; r2_score_v: {r2_score_v:.4f}'\n",
    "                    log_and_print(log_message, log_file_path)\n",
    "\n",
    "                if r2_best < r2_score_v:\n",
    "                    best_test_loss = loss_test\n",
    "                    torch.save(model.state_dict(), os.path.join(results_dir, 'best_test_model_ale_epi_.pth'))\n",
    "                    MAE_best = MAE2\n",
    "                    r2_best = r2_score_v\n",
    "                \n",
    "            loss_test_log.append(torch.mean(loss_test).item())\n",
    "        \n",
    "        model_test = UA_CNN().to(device)\n",
    "        model_test.load_state_dict(torch.load(os.path.join(results_dir, 'best_test_model_ale_epi_.pth')))\n",
    "        y_test_pre, y_test_vector, y_test_pre_log_var = model_test(x_test)\n",
    "        y_train_pre, y_train_vector, y_train_pre_log_var= model_test(x_train)\n",
    "        with torch.no_grad():\n",
    "            idx_train = torch.nonzero(y_train_sub.squeeze()!=0,as_tuple=False)\n",
    "            idx_tst = torch.nonzero(y_test_sub.squeeze()!=0,as_tuple=False)\n",
    "            y_train_sub1 = torch.index_select(y_train_sub.squeeze(), dim=0, index = idx_train.squeeze())\n",
    "            y_train_pre1 = torch.index_select(y_train_pre.squeeze(), dim=0, index = idx_train.squeeze())\n",
    "            y_train_pre_log_var1 = torch.index_select(y_train_pre_log_var.squeeze(), dim=0, index = idx_train.squeeze())\n",
    "            y_train_vector1 = torch.index_select(y_train_vector.squeeze(), dim=0, index = idx_train.squeeze())\n",
    "            y_test_pre_log_var1 = torch.index_select(y_test_pre_log_var.squeeze(), dim=0, index = idx_tst.squeeze())\n",
    "            y_test_sub1 = torch.index_select(y_test_sub.squeeze(), dim=0, index = idx_tst.squeeze())\n",
    "            y_test_pre1 = torch.index_select(y_test_pre.squeeze(), dim=0, index = idx_tst.squeeze())\n",
    "            y_test_vector1 = torch.index_select(y_test_vector.squeeze(), dim=0, index = idx_tst.squeeze())\n",
    "            \n",
    "        mse_loss_test = loss_func(y_test_pre1.squeeze(), y_test_sub1.squeeze())\n",
    "        h_loss_test = heteroscedastic_loss(y_test_sub1.squeeze(),y_test_pre1.squeeze(),y_test_pre_log_var1.squeeze())\n",
    "        loss_test = mse_loss_test + heteroscedastic_loss_coefficient * h_loss_test\n",
    "        MAE_test = mean_absolute_error(y_test_sub1.cpu().numpy().squeeze(),y_test_pre1.cpu().numpy().squeeze())\n",
    "        r2_test = r2_score(y_test_sub1.cpu().numpy().squeeze(),y_test_pre1.cpu().numpy().squeeze())\n",
    "        \n",
    "        mse_loss_train = loss_func(y_train_pre1.squeeze(), y_train_sub1.squeeze())\n",
    "        h_loss_train = heteroscedastic_loss(y_train_sub1.squeeze(),y_train_pre1.squeeze(),y_train_pre_log_var1.squeeze())\n",
    "        loss_train = mse_loss_train + heteroscedastic_loss_coefficient * h_loss_train\n",
    "        MAE_train = mean_absolute_error(y_train_sub1.cpu().numpy().squeeze(),y_train_pre1.cpu().numpy().squeeze())\n",
    "        r2_train = r2_score(y_train_sub1.cpu().numpy().squeeze(),y_train_pre1.cpu().numpy().squeeze())\n",
    "        \n",
    "        test_preds_array = np.array([[x] for x in y_test_pre1.cpu().numpy()])\n",
    "        log_and_print(f\"Shape of y_test_pre: {y_test_pre.shape}\", log_file_path)\n",
    "        log_and_print(f\"Shape of test_preds_array: {test_preds_array.shape}\", log_file_path)\n",
    "        log_and_print(f\"Shape of sum_preds: {sum_preds.shape}\", log_file_path)\n",
    "        if test_preds_array.shape != sum_preds.shape:\n",
    "            log_and_print(f\"Shape mismatch: test_preds_array.shape = {test_preds_array.shape}, sum_preds.shape = {sum_preds.shape}\", log_file_path)\n",
    "        else:\n",
    "            sum_preds += test_preds_array\n",
    "        test_pred_log_vars_array = np.array([[x] for x in y_test_pre_log_var1.cpu().numpy()])\n",
    "        test_pred_vars_array = np.exp(test_pred_log_vars_array)\n",
    "        \n",
    "        log_and_print(f\"Shape of test_pred_log_vars_array: {test_pred_log_vars_array.shape}\", log_file_path)\n",
    "        log_and_print(f\"Shape of sum_ale_uncs: {sum_ale_uncs.shape}\", log_file_path)\n",
    "        if test_pred_vars_array.shape != sum_ale_uncs.shape:\n",
    "            log_and_print(f\"Shape mismatch: test_pred_log_vars_array.shape = {test_pred_log_vars_array.shape}, sum_ale_uncs.shape = {sum_ale_uncs.shape}\", log_file_path)\n",
    "        else:\n",
    "            sum_ale_uncs += test_pred_vars_array\n",
    "        \n",
    "        log_and_print(f\"Shape of all_preds: {all_preds.shape}\", log_file_path)\n",
    "        log_and_print(f\"Shape of test_preds_array: {test_preds_array.shape}\", log_file_path)\n",
    "        if test_preds_array.shape[0] == all_preds.shape[0] and test_preds_array.shape[1] == all_preds.shape[1]:\n",
    "            all_preds[:, :, i_seed] = test_preds_array\n",
    "        else:\n",
    "            log_and_print(f\"Shape mismatch for all_preds: test_preds_array.shape = {test_preds_array.shape}, all_preds.shape = {all_preds.shape}\", log_file_path)\n",
    "            \n",
    "        labels = y_test_pre1.cpu().numpy()\n",
    "        \n",
    "        if r2_test > r2_record:\n",
    "            r2_record = r2_test\n",
    "            torch.save(y_train_sub1.squeeze(), os.path.join(results_dir, 'y_train_sub_ale_epi_.pth'))\n",
    "            torch.save(y_train_pre1.squeeze(), os.path.join(results_dir, 'y_train_pre_ale_epi_.pth'))\n",
    "            torch.save(y_test_sub1.squeeze(), os.path.join(results_dir, 'y_test_sub_ale_epi_.pth'))\n",
    "            torch.save(y_test_pre1.squeeze(), os.path.join(results_dir, 'y_test_pre_ale_epi_.pth'))  \n",
    "            torch.save(y_test_vector.squeeze(), os.path.join(results_dir, 'y_test_vector_ale_epi_.pth'))\n",
    "            torch.save(y_train_vector.squeeze(), os.path.join(results_dir, 'y_train_vector_ale_epi_.pth')) \n",
    "        \n",
    "        result_r2_test.iloc[i_seed,i+1] = r2_test\n",
    "        result_mae_test.iloc[i_seed,i+1] = MAE_test\n",
    "        result_loss_test.iloc[i_seed,i+1] = loss_test.detach().cpu().numpy()\n",
    "        result_r2_train.iloc[i_seed,i+1] = r2_train\n",
    "        result_mae_train.iloc[i_seed,i+1] = MAE_train\n",
    "        result_loss_train.iloc[i_seed,i+1] = loss_train.detach().cpu().numpy()\n",
    "        \n",
    "        i = i+1\n",
    "        \n",
    "        current_results = (f\"Seed {seed} results: R2 test = {r2_test:.4f}, MAE test = {MAE_test:.4f}, \"\n",
    "                           f\"Loss test = {loss_test:.4f}, R2 train = {r2_train:.4f}, \"\n",
    "                           f\"MAE train = {MAE_train:.4f}, Loss train = {loss_train:.4f}\")\n",
    "        log_and_print(current_results, log_file_path)\n",
    "            \n",
    "    results.append(result_r2_test)\n",
    "    results.append(result_mae_test)\n",
    "    results.append(result_loss_test)\n",
    "    results.append(result_r2_train)\n",
    "    results.append(result_mae_train)\n",
    "    results.append(result_loss_train)\n",
    "    results_all.append(results)\n",
    "torch.save(results_all, os.path.join(results_dir, 'results.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum_preds.shape)\n",
    "print(x_test_len)\n",
    "print(sum_ale_uncs.shape)\n",
    "print(sum_epi_uncs.shape)\n",
    "print(all_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_preds = sum_preds / seeds_len\n",
    "avg_preds = avg_preds.tolist()\n",
    "\n",
    "avg_ale_uncs = sum_ale_uncs / seeds_len\n",
    "avg_ale_uncs = avg_ale_uncs.tolist()\n",
    "\n",
    "avg_epi_uncs = np.var(all_preds, axis=2)\n",
    "avg_epi_uncs = avg_epi_uncs.tolist()\n",
    "print(avg_preds)\n",
    "print(avg_ale_uncs)\n",
    "print(avg_epi_uncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=  [i for i in range(len(labels))]\n",
    "y = labels\n",
    "f = avg_preds\n",
    "f = [item for sublist in f for item in sublist]\n",
    "ale_var = avg_ale_uncs\n",
    "ale_var = [item for sublist in ale_var for item in sublist]\n",
    "epi_var = avg_epi_uncs\n",
    "epi_var = [item for sublist in epi_var for item in sublist]\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "f = np.array(f)\n",
    "epi_var = np.array(epi_var)\n",
    "ale_var = np.array(ale_var)\n",
    "total_std_2 = (epi_var + ale_var)**0.5\n",
    "epi_std = epi_var**0.5\n",
    "ale_std = ale_var**0.5\n",
    "total_std = epi_std + ale_std\n",
    "abs_error = np.abs(f - y)\n",
    "mae = np.mean(abs_error)\n",
    "print(\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import uncertainty_toolbox as uct\n",
    "savefig = True\n",
    "pred_mean_list = [f]\n",
    "pred_std_list = [\n",
    "    epi_std,ale_std,total_std,total_std_2\n",
    "]\n",
    "idx_counter = 0\n",
    "for i, pred_mean in enumerate(pred_mean_list):\n",
    "    for j, pred_std in enumerate(pred_std_list):\n",
    "        mace = uct.mean_absolute_calibration_error(pred_mean, pred_std, y)\n",
    "        rmsce = uct.root_mean_squared_calibration_error(pred_mean, pred_std, y)\n",
    "        ma = uct.miscalibration_area(pred_mean, pred_std, y)\n",
    "\n",
    "        idx_counter += 1\n",
    "        print(f\"MACE: {mace}, RMSCE: {rmsce}, MA: {ma}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = results_dir + '/corr_all/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "for i, pred_mean in enumerate(pred_mean_list):\n",
    "    for j, pred_std in enumerate(pred_std_list):\n",
    "        std_name = [\"epi_std\", \"ale_std\", \"total_std\", \"total_std_2\"][j]\n",
    "        # Before recalibration\n",
    "        exp_props, obs_props = uct.get_proportion_lists_vectorized(pred_mean, pred_std, y)\n",
    "        mace = uct.mean_absolute_calibration_error(pred_mean, pred_std, y, recal_model=None)\n",
    "        rmsce = uct.root_mean_squared_calibration_error(pred_mean, pred_std, y, recal_model=None)\n",
    "        ma = uct.miscalibration_area(pred_mean, pred_std, y, recal_model=None)\n",
    "        print(\"Before Recalibration:  \", end=\"\")\n",
    "        print(\"MACE: {:.5f}, RMSCE: {:.5f}, MA: {:.5f}\".format(mace, rmsce, ma))\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "        uct.plot_calibration(pred_mean, pred_std, y, exp_props=exp_props, obs_props=obs_props, ax=ax)\n",
    "        if savefig:\n",
    "            csv_filename = f\"{output_dir}before_recal_{std_name}_{timestamp}.csv\"\n",
    "            extract_coords_to_csv(ax, csv_filename)\n",
    "            uct.viz.save_figure(f\"{output_dir}before_recal_{std_name}_{timestamp}\", \"svg\")\n",
    "\n",
    "        # After recalibration\n",
    "        recal_model = uct.iso_recal(exp_props, obs_props)\n",
    "        recal_exp_props, recal_obs_props = uct.get_proportion_lists_vectorized(pred_mean, pred_std, y, recal_model=recal_model)\n",
    "        mace = uct.mean_absolute_calibration_error(pred_mean, pred_std, y, recal_model=recal_model)\n",
    "        rmsce = uct.root_mean_squared_calibration_error(pred_mean, pred_std, y, recal_model=recal_model)\n",
    "        ma = uct.miscalibration_area(pred_mean, pred_std, y, recal_model=recal_model)\n",
    "        print(\"After Recalibration:  \", end=\"\")\n",
    "        print(\"MACE: {:.5f}, RMSCE: {:.5f}, MA: {:.5f}\".format(mace, rmsce, ma))\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "        uct.plot_calibration(pred_mean, pred_std, y, exp_props=recal_exp_props, obs_props=recal_obs_props, ax=ax)\n",
    "        if savefig:\n",
    "            csv_filename = f\"{output_dir}after_recal_{std_name}_{timestamp}.csv\"\n",
    "            extract_coords_to_csv(ax, csv_filename)\n",
    "            uct.viz.save_figure(f\"{output_dir}after_recal_{std_name}_{timestamp}\", \"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_cumulative_mae(y, f, uncertainty):\n",
    "    sorted_indices = np.argsort(uncertainty)[::-1]\n",
    "    sorted_uncertainty = uncertainty[sorted_indices]\n",
    "    sorted_y = y[sorted_indices]\n",
    "    sorted_f = f[sorted_indices]\n",
    "    \n",
    "    cumulative_mae = []\n",
    "    n = len(y)\n",
    "    for i in range(n):\n",
    "        mae_i = np.mean(np.abs(sorted_f[i:] - sorted_y[i:]))\n",
    "        cumulative_mae.append(mae_i)\n",
    "    \n",
    "    return cumulative_mae, sorted_uncertainty\n",
    "\n",
    "cumulative_mae_ale, sorted_ale_std = calculate_cumulative_mae(y, f, ale_std)\n",
    "cumulative_mae_epi, sorted_epi = calculate_cumulative_mae(y, f, epi_std)\n",
    "cumulative_mae_total, sorted_total_std = calculate_cumulative_mae(y, f, total_std)\n",
    "cumulative_mae_total_2, sorted_total_std = calculate_cumulative_mae(y, f, total_std_2)\n",
    "\n",
    "n = len(y)\n",
    "confidence_percentiles = np.arange(1, n + 1) / n\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(confidence_percentiles, cumulative_mae_ale, marker='o', linestyle='-', label='ale_std')\n",
    "plt.plot(confidence_percentiles, cumulative_mae_epi, marker='s', linestyle='-', label='epi_std')\n",
    "plt.plot(confidence_percentiles, cumulative_mae_total, marker='^', linestyle='-', label='total_std')\n",
    "plt.plot(confidence_percentiles, cumulative_mae_total_2, marker='*', linestyle='-', label='total_std_2')\n",
    "\n",
    "plt.xlabel('Confidence Percentile')\n",
    "plt.ylabel('Cumulative MAE')\n",
    "plt.title('Cumulative MAE vs. Confidence Percentile')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "def save_coords_to_csv(confidence_percentiles, cumulative_mae, uncertainty_type):\n",
    "    df = pd.DataFrame({\n",
    "        'Confidence Percentile': confidence_percentiles,\n",
    "        'Cumulative MAE': cumulative_mae\n",
    "    })\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_filename = f\"{output_dir}CumulativeMAE_vs_ConfidencePercentile_{uncertainty_type}_{timestamp}.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Coordinates saved to {csv_filename}\")\n",
    "\n",
    "save_coords_to_csv(confidence_percentiles, cumulative_mae_ale, 'ale_std')\n",
    "save_coords_to_csv(confidence_percentiles, cumulative_mae_epi, 'epi_std')\n",
    "save_coords_to_csv(confidence_percentiles, cumulative_mae_total, 'total_std')\n",
    "save_coords_to_csv(confidence_percentiles, cumulative_mae_total_2, 'total_std_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "sorted_indices = np.argsort(epi_std)[::-1]\n",
    "sorted_std = epi_std[sorted_indices]\n",
    "sorted_y = y[sorted_indices]\n",
    "sorted_f = f[sorted_indices]\n",
    "correlation, p_value = spearmanr(epi_std, abs(y - f))\n",
    "correlation_ale, p_value = spearmanr(ale_std, abs(y - f))\n",
    "correlation_total, p_value = spearmanr(total_std, abs(y - f))\n",
    "correlation_total_2, p_value = spearmanr(total_std_2, abs(y - f))\n",
    "print(\"Spearman correlation epi coefficient:\", correlation)\n",
    "print(\"Spearman correlation_ale coefficient:\", correlation_ale)\n",
    "print(\"Spearman correlation_total coefficient2:\", correlation_total)\n",
    "print(\"Spearman correlation_total_2 coefficient2:\", correlation_total_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "correlation, p_value = pearsonr(epi_std, abs(y - f))\n",
    "correlation_ale, p_value = pearsonr(ale_std, abs(y - f))\n",
    "correlation_total, p_value = pearsonr(total_std, abs(y - f))\n",
    "correlation_total_2, p_value = pearsonr(total_std_2, abs(y - f))\n",
    "print(\"Pearson correlation coefficient:\", correlation)\n",
    "print(\"Pearson correlation coefficient:\", correlation_ale)\n",
    "print(\"Pearson correlation coefficient:\", correlation_total)\n",
    "print(\"Pearson correlation coefficient:\", correlation_total_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
