{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    " \n",
    "parent_dir = './'\n",
    "physical_para = pd.read_csv('./data/data_no_header.csv', header=None, low_memory=False)\n",
    "XRD_descriptor = pd.read_csv('./data/XRD_dense_descriptor_new_pi_4.csv', header=None, low_memory=False)\n",
    "data = pd.concat([XRD_descriptor, physical_para.iloc[:,11]], axis=1)\n",
    "seeds = [199228, 302675, 257057, 320858, 844620, 298933, 681403, 690678]\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--num_folds', type=int, default=1,\n",
    "                help='Number of folds when performing cross validation')\n",
    "parser.add_argument('--ensemble_size', type=int, default=3,\n",
    "                        help='Number of models in ensemble')\n",
    "parser.add_argument('--show_individual_scores', action='store_true', default=False,\n",
    "                help='Show all scores for individual targets, not just average, at the end')\n",
    "parser.add_argument('--task_inds', type=int, default=[], nargs='+',\n",
    "                help='Indices of tasks you want to train on.')\n",
    "# Active Learning Arguments\n",
    "parser.add_argument('--al_init_ratio', type=float, default=0.1,\n",
    "                    help='Percent of training data to use on first active learning iteration')\n",
    "parser.add_argument('--al_end_ratio', type=float, default=None,\n",
    "                    help='Fraction of total data To stop active learning early. By default, explore full train data')\n",
    "parser.add_argument('--num_al_loops', type=int, default=8,\n",
    "                    help='Number of active learning loops to add new data')\n",
    "parser.add_argument('--al_topk', type=int, default=25,\n",
    "                    help='Top-K acquired molecules to consider during active learning')\n",
    "parser.add_argument('--al_std_mult', type=float, default=1,\n",
    "                    help='Multiplier for std in lcb acquisition')\n",
    "parser.add_argument('--al_step_scale', type=str, default=\"linear\",\n",
    "                    help='scale of spacing for active learning steps (log, linear)')\n",
    "parser.add_argument('--acquire_min', action='store_true',\n",
    "                    help='if we should acquire min or max score molecules')\n",
    "parser.add_argument('--al_strategy', type=str, nargs='+',\n",
    "                    choices=[\"random\",\n",
    "                            \"explorative_greedy\", \"explorative_sample\",\n",
    "                            \"score_greedy\", \"score_sample\",\n",
    "                            \"exploit\", \"exploit_ucb\", \"exploit_lcb\", \"exploit_ts\"],\n",
    "                    default=[\"explorative_greedy\"],\n",
    "                    help='Strategy for active learning regime')\n",
    "parser.add_argument('--use_std', action='store_true', default=False,\n",
    "                    help='Use std for evidence during active learning')\n",
    "args = parser.parse_args([])\n",
    "args.save_dir =  parent_dir + 'results/m13/'\n",
    "args.epochs = 1000\n",
    "args.al_strategy = ['explor_total','explor_total_2','score_greedy','random','exploit_ucb','exploit_lcb','exploit_ts']\n",
    "args.LR = 0.001\n",
    "args.mb_size = 20\n",
    "heteroscedastic_loss_coefficient = 1e-3\n",
    "grid_point = [12,52]\n",
    "import torch\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heteroscedastic_loss(true, mean, log_var):\n",
    "    \"\"\"\n",
    "    Compute the heteroscedastic loss for regression.\n",
    "\n",
    "    :param true: A list of true values.\n",
    "    :param mean: A list of means (output predictions).\n",
    "    :param log_var: A list of logvars (log of predicted variances).\n",
    "    :return: Computed loss.\n",
    "    \"\"\"\n",
    "    precision = torch.exp(-log_var)\n",
    "    loss = precision * (true - mean)**2 + log_var\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from utils import makedirs\n",
    "        \n",
    "if args.save_dir is not None:\n",
    "    timestamp = datetime.now().strftime(\"%y%m%d-%H%M%S%f\")\n",
    "    dataset = 'XRD'\n",
    "    log_path = \"{}_{}\".format(timestamp, dataset)\n",
    "    args.save_dir = os.path.join(args.save_dir, log_path)\n",
    "    if os.path.exists(args.save_dir):\n",
    "        num_ctr = 0\n",
    "        while (os.path.exists(f\"{args.save_dir}_{num_ctr}\")):\n",
    "            num_ctr += 1\n",
    "        args.save_dir = f\"{args.save_dir}_{num_ctr}\"\n",
    "    makedirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.iloc[:, :4096], data.iloc[:, 4096], test_size=0.3, random_state=500)\n",
    "print(\"Train data:\", len(x_train))\n",
    "print(\"Test data:\", len(x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UA_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UA_CNN, self).__init__()#N*2*4096\n",
    "        \n",
    "        self.partition_1 = nn.ModuleList([nn.Conv1d(in_channels=1,out_channels=2,kernel_size=3,padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool1d(2,2),#N*4*1024\n",
    "                        nn.Conv1d(in_channels=2,out_channels=4,kernel_size=3,padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool1d(2,2),#N*4*512\n",
    "                        nn.Conv1d(in_channels=4,out_channels=2,kernel_size=3,padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool1d(2,2),#N*2*256\n",
    "                        nn.Conv1d(in_channels=2,out_channels=1,kernel_size=3,padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool1d(2,2)#N*1*128\n",
    "                        ])\n",
    "        self.partition_2 = nn.ModuleList([nn.Conv1d(in_channels=1,out_channels=2,kernel_size=3,padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool1d(2,2),#N*4*1024\n",
    "                        nn.Conv1d(in_channels=2,out_channels=4,kernel_size=3,padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool1d(2,2),#N*4*512\n",
    "                        nn.Conv1d(in_channels=4,out_channels=2,kernel_size=3,padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool1d(2,2),#N*2*256\n",
    "                        nn.Conv1d(in_channels=2,out_channels=1,kernel_size=3,padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool1d(2,2)#N*1*128\n",
    "                        ])\n",
    "        # self.dropout_layer = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(256,64)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64,32)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(32,8)\n",
    "        self.relu7 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(8,1)\n",
    "        \n",
    "        self.fc1_var = nn.Linear(256,64)\n",
    "        self.relu5_var = nn.ReLU()\n",
    "        self.fc2_var = nn.Linear(64,32)\n",
    "        self.relu6_var = nn.ReLU()\n",
    "        self.fc3_var = nn.Linear(32,8)\n",
    "        self.relu7_var = nn.ReLU()\n",
    "        self.fc4_var = nn.Linear(8,1)\n",
    "    \n",
    "    def forward(self, x, gp):\n",
    "        # out = self.lanorm(x)\n",
    "        x1 = x[:,:,:gp[0]*64]\n",
    "        x2 = x[:,:,gp[0]*64:]\n",
    "        \n",
    "        for item in self.partition_1:\n",
    "            x1 = item(x1)\n",
    "        for item in self.partition_2:\n",
    "            x2 = item(x2)\n",
    "        \n",
    "        x = torch.cat([x1,x2],dim=2)\n",
    "\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu5(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu6(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu7(out)\n",
    "        out = self.fc4(out)\n",
    "        \n",
    "        out_var = self.fc1_var(x)\n",
    "        out_var = self.relu5_var(out_var)\n",
    "        out_var = self.fc2_var(out_var)\n",
    "        out_var = self.relu6_var(out_var)\n",
    "        out_var = self.fc3_var(out_var)\n",
    "        out_var = self.relu7_var(out_var)\n",
    "        out_var = self.fc4_var(out_var)\n",
    "        return out, x, out_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X_train, Y_train, mini_batch_size = 10):                           \n",
    "    mini_batches = []\n",
    "    X_train = torch.split(X_train, mini_batch_size)\n",
    "    Y_train = torch.split(Y_train, mini_batch_size)\n",
    "    for i in np.arange(len(X_train)):\n",
    "        mini_batch = (X_train[i],Y_train[i])\n",
    "        mini_batches.append(mini_batch)\n",
    "    return mini_batches\n",
    "def setup_seed(seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os \n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from utils import makedirs\n",
    "\n",
    "def run_training(x_train,y_train,x_test,y_test,args,loss_func):\n",
    "    print(\"Train data:\", len(x_train))\n",
    "    print(\"Test data:\", len(x_test))\n",
    "    \n",
    "    x_train = torch.from_numpy(x_train.values).float().to(device)\n",
    "    x_test = torch.from_numpy(x_test.values).float().to(device)\n",
    "    y_train = torch.from_numpy(y_train.values).float().to(device)\n",
    "    y_test = torch.from_numpy(y_test.values).float().to(device)\n",
    "\n",
    "    x_train = torch.unsqueeze(x_train, 1)\n",
    "    x_test = torch.unsqueeze(x_test, 1)\n",
    "\n",
    "    ensemble_models = []\n",
    "    for model_idx in range(args.ensemble_size):\n",
    "        seed = seeds[model_idx]\n",
    "        model_idx_result_dir = os.path.join(args.save_dir, f'model_{model_idx}')\n",
    "        makedirs(model_idx_result_dir)\n",
    "        model_idx = run_training_single_model(x_train,y_train,x_test,y_test,seed,args,model_idx_result_dir,model_idx,loss_func)\n",
    "        ensemble_models.append(model_idx)\n",
    "    return ensemble_models\n",
    "\n",
    "def run_training_single_model(x_train,y_train,x_test,y_test,seed,args,model_idx_result_dir,model_idx,loss_func):\n",
    "    setup_seed(seed)\n",
    "    input_size, feature_size = x_train.shape[0], x_train.shape[1]\n",
    "    model = UA_CNN().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.LR)\n",
    "    y_train_sub = y_train\n",
    "    y_test_sub = y_test\n",
    "    r2_best = -math.inf\n",
    "    MAE_best = 0\n",
    "    for epoch in range(args.epochs):\n",
    "        epoch_loss = 0\n",
    "        num_minibatches = int(input_size / args.mb_size) + 1\n",
    "        minibatches = random_mini_batches(x_train, y_train_sub, args.mb_size)\n",
    "        model.train()\n",
    "        for minibatch in minibatches:\n",
    "            batch_x, batch_y  = minibatch\n",
    "            batch_y_pre, _, batch_y_pre_var = model(batch_x,grid_point)\n",
    "            idx = torch.nonzero(batch_y.squeeze()!=0,as_tuple=False)\n",
    "            batch_y_pre1 = torch.index_select(batch_y_pre.squeeze(), dim=0, index = idx.squeeze())\n",
    "            batch_y1 = torch.index_select(batch_y.squeeze(), dim=0, index = idx.squeeze())\n",
    "            batch_y_pre_log_var1 = torch.index_select(batch_y_pre_var.squeeze(), dim=0, index = idx.squeeze())\n",
    "            mse_loss = loss_func(batch_y_pre1.squeeze(), batch_y1.squeeze())    \n",
    "            h_loss = heteroscedastic_loss(batch_y1.squeeze(),batch_y_pre1.squeeze(),batch_y_pre_log_var1.squeeze())\n",
    "            loss = mse_loss + heteroscedastic_loss_coefficient * h_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss = epoch_loss + (loss / num_minibatches)\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_test_pre, _, y_test_pre_var = model(x_test,grid_point)\n",
    "            idx_test = torch.nonzero(y_test_sub.squeeze()!=0,as_tuple=False)\n",
    "            y_test_pre1 = torch.index_select(y_test_pre.squeeze(), dim=0, index = idx_test.squeeze())\n",
    "            y_test_sub1 = torch.index_select( y_test_sub.squeeze(), dim=0, index = idx_test.squeeze())        \n",
    "            y_test_pre_log_var1 = torch.index_select( y_test_pre_var.squeeze(), dim=0, index = idx_test.squeeze())        \n",
    "            mse_loss_test = loss_func(y_test_pre1.squeeze(), y_test_sub1.squeeze())\n",
    "            h_loss_test = heteroscedastic_loss(y_test_sub1.squeeze(), y_test_pre1.squeeze(), y_test_pre_log_var1.squeeze())\n",
    "            loss_test = mse_loss_test + heteroscedastic_loss_coefficient * h_loss_test\n",
    "            MAE2 = mean_absolute_error(y_test_sub1.cpu().numpy().squeeze(),y_test_pre1.cpu().numpy().squeeze())\n",
    "            MSE = mean_squared_error(y_test_sub1.cpu().numpy().squeeze(), y_test_pre1.cpu().numpy().squeeze())\n",
    "            r2_score_v =  r2_score(y_test_sub1.cpu().numpy().squeeze(),y_test_pre1.cpu().numpy().squeeze()) \n",
    "            if r2_best < r2_score_v:\n",
    "                best_test_loss = loss_test\n",
    "                torch.save(model.state_dict(), os.path.join(model_idx_result_dir, 'best_test_model.pth'))\n",
    "                MAE_best = MAE2\n",
    "                r2_best = r2_score_v\n",
    "            if (epoch+1)%100==0:\n",
    "                print('Iter-{}; Total loss: {:.4}; MAE2: {:.4}; MSE: {:.4} r2_score_v: {:.4}'.format(epoch, loss_test.item(), MAE2, MSE, r2_score_v))\n",
    "\n",
    "    \n",
    "    model_test = UA_CNN().to(device)\n",
    "    model_test.load_state_dict(torch.load(os.path.join(model_idx_result_dir, 'best_test_model.pth')))\n",
    "    print(f\"Done with model {model_idx}\")\n",
    "    return model_test\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models,loss_func,x_test,y_test,test=True):\n",
    "    x_test = torch.from_numpy(x_test.values).float().to(device)\n",
    "    y_test = torch.from_numpy(y_test.values).float().to(device)\n",
    "    x_test = torch.unsqueeze(x_test, 1)\n",
    "    y_test_sub = y_test\n",
    "    \n",
    "    predictions = []\n",
    "    sum_preds = np.zeros((len(x_test), 1))\n",
    "    sum_ale_uncs = np.zeros((len(x_test), 1))\n",
    "    all_preds = np.zeros((len(x_test), 1, args.ensemble_size))\n",
    "    \n",
    "    idx_tst = torch.nonzero(y_test_sub.squeeze()!=0,as_tuple=False)\n",
    "    y_test_sub1 = torch.index_select(y_test_sub.squeeze(), dim=0, index = idx_tst.squeeze())\n",
    "        \n",
    "    for model_idx, model in enumerate(models):\n",
    "        with torch.no_grad():\n",
    "            y_test_pre, _, y_test_pre_var= model(x_test,grid_point)\n",
    "        y_test_pre1 = torch.index_select(y_test_pre.squeeze(), dim=0, index = idx_tst.squeeze())\n",
    "        y_test_pre_log_var1 = torch.index_select(y_test_pre_var.squeeze(), dim=0, index = idx_tst.squeeze())\n",
    "        predictions.append(y_test_pre1.tolist())\n",
    "  \n",
    "        test_preds_array = np.array([[x] for x in y_test_pre1.cpu().numpy()])\n",
    "        sum_preds += np.array(test_preds_array)\n",
    "        \n",
    "        test_pred_log_vars_array = np.array([[x] for x in y_test_pre_log_var1.cpu().numpy()])\n",
    "        test_pred_vars_array = np.exp(test_pred_log_vars_array)\n",
    "        sum_ale_uncs += np.array(test_pred_vars_array)\n",
    "        \n",
    "        all_preds[:, :, model_idx] = test_preds_array\n",
    "        \n",
    "        mse_loss_test = loss_func(y_test_pre1.squeeze(), y_test_sub1.squeeze())\n",
    "        h_loss_test = heteroscedastic_loss(y_test_sub1.squeeze(),y_test_pre1.squeeze(),y_test_pre_log_var1.squeeze())\n",
    "        loss_test = mse_loss_test + heteroscedastic_loss_coefficient * h_loss_test\n",
    "        MAE_test = mean_absolute_error(y_test_sub1.cpu().numpy().squeeze(),y_test_pre1.cpu().numpy().squeeze())\n",
    "        MSE = mean_squared_error(y_test_sub1.cpu().numpy().squeeze(), y_test_pre1.cpu().numpy().squeeze())\n",
    "        r2_test = r2_score(y_test_sub1.cpu().numpy().squeeze(),y_test_pre1.cpu().numpy().squeeze())\n",
    "        \n",
    "        if args.show_individual_scores:\n",
    "            # Individual test scores\n",
    "            print('Model {} test Total loss: {:.4}; MAE2: {:.4}; MSE: {:.4}, r2_score_v: {:.4}'.format(model_idx, loss_test.item(), MAE_test, MSE, r2_test))\n",
    "                    \n",
    "    ensemble_predictions = np.mean(predictions, axis=0)\n",
    "    ensemble_MAE_test = mean_absolute_error(y_test_sub1.cpu().numpy().squeeze(),ensemble_predictions.squeeze())\n",
    "    ensemble_MSE_test = mean_squared_error(y_test_sub1.cpu().numpy().squeeze(),ensemble_predictions.squeeze())\n",
    "    ensemble_r2_test = r2_score(y_test_sub1.cpu().numpy().squeeze(),ensemble_predictions.squeeze())\n",
    "    if test == True:\n",
    "        print('Ensemble model test Total loss: {:.4}; MAE2: {:.4}; MSE: {:.4},  r2_score_v: {:.4}'.format(loss_test.item(), ensemble_MAE_test,ensemble_MSE_test, ensemble_r2_test))\n",
    "    else:\n",
    "        print('Ensemble model train Total loss: {:.4}; MAE2: {:.4}; MSE: {:.4},  r2_score_v: {:.4}'.format(loss_test.item(), ensemble_MAE_test,ensemble_MSE_test, ensemble_r2_test))\n",
    "            \n",
    "    avg_preds = sum_preds / args.ensemble_size\n",
    "    avg_preds = avg_preds.tolist()\n",
    "\n",
    "    avg_ale_uncs = sum_ale_uncs / args.ensemble_size\n",
    "    avg_ale_uncs = avg_ale_uncs.tolist()\n",
    "\n",
    "    avg_epi_uncs = np.var(all_preds, axis=2)\n",
    "    # 对每个元素取平方根\n",
    "    epi_std = np.sqrt(avg_epi_uncs)\n",
    "    ale_std = np.sqrt(avg_ale_uncs)\n",
    "    total_std = epi_std + ale_std\n",
    "    total_std2 = np.sqrt(avg_epi_uncs+avg_ale_uncs)\n",
    "\n",
    "    epi_std = epi_std.tolist()\n",
    "    ale_std = ale_std.tolist()\n",
    "    total_std = total_std.tolist()\n",
    "    total_std2 = total_std2.tolist()\n",
    "    return ensemble_MAE_test,ensemble_MSE_test,ensemble_r2_test,ensemble_predictions,epi_std,ale_std,total_std,total_std2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "setup_seed(42)\n",
    "\n",
    "results_root = Path(args.save_dir)\n",
    "Path(results_root).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    columns=[\"Trial\", \"Train Data Ratio\", \"Score_mae\",\"Score_mse\", \"Uncertainty\", \"Entropy\"])\n",
    "\n",
    "### Define active learning step variables and subsample the tasks\n",
    "n_total = len(x_train)\n",
    "n_sample = n_total\n",
    "n_loops = args.num_al_loops\n",
    "\n",
    "### Change active learning n_sample for early stopping\n",
    "if args.al_end_ratio is not None:\n",
    "    if args.al_end_ratio > 1:\n",
    "        raise ValueError(\"Arg al_end_ratio must be less than train size\")\n",
    "    total_data = len(x_train) + len(x_test)\n",
    "    early_stop_num = int(n_total * args.al_end_ratio)\n",
    "    n_sample = early_stop_num\n",
    "\n",
    "n_start = int(n_total * args.al_init_ratio)\n",
    "\n",
    "train_subset_inds_start = np.random.choice(n_total, n_start, replace=False)\n",
    "\n",
    "print(f\"Ratio targets 0/1: {np.nanmean(np.array(y_train), axis=0)}\")\n",
    "\n",
    "### Compute the number of samples to use at each step of active learning\n",
    "if args.al_step_scale == \"linear\":\n",
    "    n_samples_per_run = np.linspace(n_start, n_sample, n_loops)\n",
    "elif args.al_step_scale == \"log\":\n",
    "    n_samples_per_run = np.logspace(np.log10(n_start), np.log10(n_sample), n_loops)\n",
    "else:\n",
    "    raise ValueError(f\"unknown args.al_step_scale = {args.al_step_scale}\")\n",
    "n_samples_per_run = np.round(n_samples_per_run).astype(int)\n",
    "\n",
    "loss_func = nn.MSELoss()    \n",
    "for strategy in args.al_strategy:\n",
    "    train_subset_inds = np.copy(train_subset_inds_start)\n",
    "    \n",
    "    tic_time = time.time() # grab the current time for logging\n",
    "    i_trial = 0\n",
    "    \n",
    "    ### Main active learning loop\n",
    "    for i in range(n_loops):\n",
    "        print(f\"===> [{strategy}] Running trial {i_trial} with {n_samples_per_run[i]} samples\")\n",
    "        current_x_train_data = x_train.iloc[train_subset_inds]\n",
    "        current_y_train_data = y_train.iloc[train_subset_inds]\n",
    "        \n",
    "        ### Train with the data subset, return the best models\n",
    "        models = run_training(\n",
    "            current_x_train_data, current_y_train_data,x_test,y_test, args, loss_func)\n",
    "\n",
    "        if \"explorative\" in strategy or \"explor\" in strategy or \"score\" in strategy or \"exploit\" in strategy:\n",
    "            ensemble_MAE_train, ensemble_MSE_train, ensemble_r2_train, all_train_preds, all_train_std,all_train_ale_std,all_train_total_std,all_train_total_std_2 = evaluate_models(models,loss_func,x_train,y_train,test=False)\n",
    "            sq_error = np.square(np.array(y_train) - all_train_preds)\n",
    "            rmse = np.sqrt( sq_error.astype(np.float32)) \n",
    "        \n",
    "            mean_uncertainty = np.mean(all_train_std, axis=1)\n",
    "            mean_ale_uncertainty = np.mean(all_train_ale_std, axis=1)\n",
    "            mean_total_uncertainty = np.mean(all_train_total_std, axis=1)\n",
    "            mean_total_uncertainty_2 = np.mean(all_train_total_std_2, axis=1)\n",
    "            if \"explorative_greedy\" == strategy:\n",
    "                per_sample_weight = mean_uncertainty\n",
    "            elif \"explor_ale\" == strategy:\n",
    "                per_sample_weight = mean_ale_uncertainty\n",
    "            elif \"explor_total\"  == strategy:\n",
    "                per_sample_weight = mean_total_uncertainty\n",
    "            elif \"explor_total_2\"  == strategy:\n",
    "                per_sample_weight = mean_total_uncertainty_2\n",
    "            elif \"score_greedy\" == strategy:\n",
    "                per_sample_weight = rmse\n",
    "            elif \"exploit\" in strategy:\n",
    "                per_sample_weight = all_train_preds\n",
    "                # Reverse and make sure weights (preds) are positive\n",
    "                if args.acquire_min:\n",
    "                    per_sample_weight *= -1\n",
    "\n",
    "                std_mult = args.al_std_mult\n",
    "                if \"_lcb\" in strategy: # lower confidence bound\n",
    "                    per_sample_weight += -std_mult * mean_uncertainty\n",
    "                elif \"_ucb\" in strategy: # upper confidence bound\n",
    "                    per_sample_weight += +std_mult * mean_uncertainty\n",
    "                elif \"_ts\" in strategy: # thompson sampling\n",
    "                    per_sample_weight = np.random.normal(\n",
    "                        per_sample_weight, mean_uncertainty)\n",
    "\n",
    "                per_sample_weight -= per_sample_weight.min()\n",
    "                \n",
    "            ### Save all the smiles along with their uncertainties/errors\n",
    "            train_subset_mask = np.zeros((n_total,))\n",
    "            train_subset_mask[train_subset_inds] = 1\n",
    "            x_train_index = np.array(x_train.index)\n",
    "            df_scores = pd.DataFrame(data={\n",
    "                        \"X_train\": x_train_index,\n",
    "                        \"Uncertainty\": mean_uncertainty,\n",
    "                        \"Ale_Uncertainty\": mean_ale_uncertainty,\n",
    "                        \"Total_Uncertainty\": mean_total_uncertainty,\n",
    "                        \"Total_Uncertainty_2\": mean_total_uncertainty_2,\n",
    "                        \"Error\": rmse,\n",
    "                        \"TrainInds\": train_subset_mask\n",
    "                    })\n",
    "            Path(os.path.join(results_root, \"tracks\")).mkdir(\n",
    "                    parents=True, exist_ok=True)\n",
    "            df_scores.to_csv(os.path.join(results_root, \"tracks\",\n",
    "                f\"{strategy}_step_{i}_{tic_time}.csv\"))\n",
    "        \n",
    "        elif strategy == \"random\":\n",
    "            per_sample_weight = np.ones((n_total,)) # uniform\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown active learning strategy {strategy}\")\n",
    "        \n",
    "        ensemble_MAE_test,ensemble_MSE_test,ensemble_r2_test,test_preds,test_std,test_ale_std,test_total_std,test_total_std_2 = evaluate_models(models,loss_func,x_test,y_test,test=True)\n",
    "        \n",
    "        ### Compute the top-k percent acquired\n",
    "        # Grab the indicies that are in the top-k of only the training data\n",
    "        top_k_scores_in_pool = np.sort(y_train)\n",
    "        top_k_scores_in_pool = top_k_scores_in_pool[:args.al_topk] \\\n",
    "                                if args.acquire_min else \\\n",
    "                                top_k_scores_in_pool[-args.al_topk:]\n",
    "\n",
    "        top_k_scores_in_selection = np.sort(current_y_train_data)\n",
    "        top_k_scores_in_selection = top_k_scores_in_selection[:args.al_topk] \\\n",
    "                                if args.acquire_min else \\\n",
    "                                top_k_scores_in_selection[-args.al_topk:]\n",
    "\n",
    "        # Find the overlap in indicies with our already acquired data points\n",
    "        selection_overlap = np.in1d(top_k_scores_in_selection,\n",
    "                                    top_k_scores_in_pool)\n",
    "\n",
    "        # Compute the percent overlap\n",
    "        percent_top_k_overlap = np.mean(selection_overlap) * 100\n",
    "        ###\n",
    "        \n",
    "        df = pd.concat([df, pd.DataFrame([{\n",
    "            'Train Data Ratio': n_samples_per_run[i] / float(n_total),\n",
    "            'Score_mae': np.mean(ensemble_MAE_test),\n",
    "            'Score_mse': np.mean(ensemble_MSE_test),\n",
    "            'Score_R2': np.mean(ensemble_r2_test),\n",
    "            'Uncertainty': np.mean(test_std),\n",
    "            'Ale_Uncertainty': np.mean(test_ale_std),\n",
    "            'Total_Uncertainty': np.mean(test_total_std),\n",
    "            'Total_Uncertainty_2': np.mean(test_total_std_2),\n",
    "            'Standard Deviation': np.mean(test_std),\n",
    "            'Trial': i_trial,\n",
    "            'Strategy': strategy,\n",
    "            'Top_k':percent_top_k_overlap,\n",
    "            'Tasks': 'XRD',\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "        ### Save the complete test performance (including uncs) to log\n",
    "        test_error = test_preds - np.array(y_test)\n",
    "        log_data_dict = {\"Error_0\": test_error}\n",
    "        \n",
    "        x_test_index = np.array(x_test.index)\n",
    "\n",
    "        log_data_dict.update({\n",
    "            \"X_test\": x_test_index,\n",
    "            \"Uncertainty\": np.mean(test_std, 1),\n",
    "            \"Ale_Uncertainty\": np.mean(test_ale_std, 1),\n",
    "            \"Total_Uncertainty\": np.mean(test_total_std, 1),\n",
    "            \"Total_Uncertainty_2\": np.mean(test_total_std_2, 1),\n",
    "            \"Std\": np.mean(test_std, 1),\n",
    "            \"TopK\": percent_top_k_overlap,\n",
    "            \"Train Data Ratio\": n_samples_per_run[i]/float(n_total),\n",
    "        })\n",
    "        df_test_log = pd.DataFrame(data=log_data_dict)\n",
    "        Path(os.path.join(results_root, \"scores\")).mkdir(\n",
    "            parents=True, exist_ok=True)\n",
    "        df_test_log.to_csv(os.path.join(results_root, \"scores\",\n",
    "            f\"{strategy}_step_{i}_{tic_time}.csv\"))\n",
    "\n",
    "        n_top = args.al_topk  # Use a parameter to dynamically determine how many top values to select\n",
    "        top_indices = np.argsort(test_preds)[:n_top] if args.acquire_min else np.argsort(test_preds)[-n_top:]\n",
    "        top_kd_values = np.array(y_test)[top_indices]\n",
    "        top_pred_values = test_preds[top_indices]\n",
    "\n",
    "        df_top = pd.DataFrame({\n",
    "            'Strategy': strategy,\n",
    "            'Trial': i_trial,\n",
    "            'Top_Kd_Values': top_kd_values,\n",
    "            'Top_Pred_Values': top_pred_values\n",
    "        })\n",
    "        df_top.to_csv(os.path.join(results_root, f\"{strategy}_top_{n_top}_kd_{i}_{tic_time}.csv\"), index=False)\n",
    "        \n",
    "        print(\"Percent top-k = {}\".format(round(percent_top_k_overlap, 2)))\n",
    "\n",
    "        ### Add new samples to training set\n",
    "        n_add = n_samples_per_run[min(i+1, n_loops-1)] - n_samples_per_run[i]\n",
    "        if n_add > 0: # n_add = 0 on the last iteration, when we are done\n",
    "\n",
    "            # Probability of sampling a new point, depends on the weight\n",
    "            per_sample_prob = deepcopy(per_sample_weight)\n",
    "\n",
    "            # Exclude data we've already trained with, and normalize to probability\n",
    "            per_sample_prob[train_subset_inds] = 0.0\n",
    "            per_sample_prob = per_sample_prob / per_sample_prob.sum()\n",
    "\n",
    "            # Sample accordingly and add to our training inds\n",
    "            if \"sample\" in strategy:\n",
    "                train_inds_to_add = np.random.choice(n_total, size=n_add, p=per_sample_prob, replace=False)\n",
    "            else:\n",
    "                # greedy, just pick the highest probability indicies\n",
    "                inds_sorted = np.argsort(per_sample_prob) # smallest to largest\n",
    "                train_inds_to_add = inds_sorted[-n_add:] # grab the last k inds\n",
    "\n",
    "            # Add the indices to the training set\n",
    "            train_subset_inds = np.append(train_subset_inds, train_inds_to_add)\n",
    "        i_trial = i_trial+1\n",
    "        del models\n",
    "        torch.cuda.empty_cache()    \n",
    "        \n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S.%f\")\n",
    "csv_filename = f\"{timestamp}_{args.task_inds}.csv\"\n",
    "csv_save_path = os.path.join(results_root, csv_filename)\n",
    "df.to_csv(csv_save_path)\n",
    "print(f\"Done with all folds and saved into {results_root}\")\n",
    "print(f\"CSV file saved at: {csv_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "df = pd.read_csv(csv_save_path)\n",
    "\n",
    "train_data_ratio = df[\"Train Data Ratio\"]\n",
    "mae = df[\"Score_mae\"]\n",
    "\n",
    "strategies = df[\"Strategy\"].unique()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "all_coords_mae = []\n",
    "\n",
    "for strategy in strategies:\n",
    "    strategy_df = df[df[\"Strategy\"] == strategy]\n",
    "    plt.plot(strategy_df[\"Train Data Ratio\"], strategy_df[\"Score_mae\"], label=strategy)\n",
    "    coords_mae = strategy_df[[\"Train Data Ratio\", \"Score_mae\"]].copy()\n",
    "    coords_mae[\"Strategy\"] = strategy\n",
    "    all_coords_mae.append(coords_mae)\n",
    "\n",
    "plt.xlabel(\"Train Data Ratio\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.title(\"MAE vs Train Data Ratio for Different Strategies\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "all_coords_mae_df = pd.concat(all_coords_mae)\n",
    "mae_coords_save_path = os.path.join(args.save_dir, \"mae_vs_train_data_ratio_all_coordinates.csv\")\n",
    "all_coords_mae_df.to_csv(mae_coords_save_path, index=False)\n",
    "\n",
    "print(f\"All MAE coordinates saved to: {mae_coords_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(csv_save_path)\n",
    "train_data_ratio = df[\"Train Data Ratio\"]\n",
    "mse = df[\"Score_mse\"]\n",
    "strategies = df[\"Strategy\"].unique()\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "all_coords = []\n",
    "\n",
    "for strategy in strategies:\n",
    "    strategy_df = df[df[\"Strategy\"] == strategy]\n",
    "    plt.plot(strategy_df[\"Train Data Ratio\"], strategy_df[\"Score_mse\"], label=strategy)\n",
    "    coords = strategy_df[[\"Train Data Ratio\", \"Score_mse\"]].copy()\n",
    "    coords[\"Strategy\"] = strategy\n",
    "    all_coords.append(coords)\n",
    "\n",
    "plt.xlabel(\"Train Data Ratio\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"MSE vs Train Data Ratio for Different Strategies\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "all_coords_df = pd.concat(all_coords)\n",
    "coords_save_path = os.path.join(args.save_dir, \"mse_vs_train_data_ratio_all_coordinates.csv\")\n",
    "all_coords_df.to_csv(coords_save_path, index=False)\n",
    "\n",
    "print(f\"All coordinates saved to: {coords_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(csv_save_path)\n",
    "train_data_ratio = df[\"Train Data Ratio\"]\n",
    "strategies = df[\"Strategy\"].unique()\n",
    "strategies = ['explorative_greedy', 'explor_total' ,'explor_total_2' ,'explor_ale',\n",
    " 'score_greedy','random' ]\n",
    "print(strategies)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "all_coords = []\n",
    "\n",
    "for strategy in strategies:\n",
    "    strategy_df = df[df[\"Strategy\"] == strategy]\n",
    "    plt.plot(strategy_df[\"Train Data Ratio\"], strategy_df[\"Score_R2\"], label=strategy)\n",
    "    coords = strategy_df[[\"Train Data Ratio\", \"Score_R2\"]].copy()\n",
    "    coords[\"Strategy\"] = strategy\n",
    "    all_coords.append(coords)\n",
    "\n",
    "plt.xlabel(\"Train Data Ratio\")\n",
    "plt.ylabel(\"R2\")\n",
    "plt.title(\"R2 vs Train Data Ratio for Different Strategies\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "all_coords_df = pd.concat(all_coords)\n",
    "coords_save_path = os.path.join(args.save_dir, \"R2_vs_train_data_ratio_all_coordinates.csv\")\n",
    "all_coords_df.to_csv(coords_save_path, index=False)\n",
    "\n",
    "print(f\"All coordinates saved to: {coords_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random_data = df[df[\"Strategy\"] == \"random\"]\n",
    "strategies = df[\"Strategy\"].unique()\n",
    "strategies_data = {strategy: df[df[\"Strategy\"] == strategy] for strategy in strategies}\n",
    "\n",
    "random_mse = random_data[\"Score_mse\"].values\n",
    "sample_efficiency = {}\n",
    "all_coords = []\n",
    "\n",
    "for strategy, data in strategies_data.items():\n",
    "    mse = data[\"Score_mse\"].values\n",
    "    efficiency = (random_mse - mse) / random_mse * 100\n",
    "    sample_efficiency[strategy] = efficiency\n",
    "    coords = pd.DataFrame({\n",
    "        \"Train Data Ratio\": data[\"Train Data Ratio\"],\n",
    "        \"Sample Efficiency (%)\": efficiency,\n",
    "        \"Strategy\": strategy\n",
    "    })\n",
    "    all_coords.append(coords)\n",
    "\n",
    "train_data_ratio = random_data[\"Train Data Ratio\"]\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for strategy in sample_efficiency.keys():\n",
    "    plt.plot(train_data_ratio, sample_efficiency[strategy], marker='o', label=strategy)\n",
    "\n",
    "plt.plot(train_data_ratio, [0] * len(train_data_ratio), linestyle='--', color='gray', label='Random')\n",
    "\n",
    "plt.xlabel(\"Train Data Ratio\")\n",
    "plt.ylabel(\"Sample Efficiency (%)\")\n",
    "plt.title(\"Change in Sample Efficiency for Different Acquisition Strategies\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "all_coords_df = pd.concat(all_coords)\n",
    "coords_save_path = os.path.join(args.save_dir, \"change_in_sample_efficiency_all_coordinates.csv\")\n",
    "all_coords_df.to_csv(coords_save_path, index=False)\n",
    "\n",
    "print(f\"All coordinates saved to: {coords_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "strategies = df[\"Strategy\"].unique()\n",
    "\n",
    "strategy_data = {}\n",
    "for strategy in strategies:\n",
    "    strategy_data[strategy] = df[df['Strategy'] == strategy]\n",
    "top_k_data = {strategy: strategy_data[strategy]['Top_k'].values for strategy in strategies}\n",
    "x_values = df['Train Data Ratio'].unique()\n",
    "all_coords = []\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for strategy in strategies:\n",
    "    plt.plot(x_values, top_k_data[strategy], marker='o', label=strategy.replace('_', ' ').title())\n",
    "    coords = pd.DataFrame({\n",
    "        \"Train Data Ratio\": x_values,\n",
    "        \"% of top-k scores found\": top_k_data[strategy],\n",
    "        \"Strategy\": strategy\n",
    "    })\n",
    "    all_coords.append(coords)\n",
    "\n",
    "plt.xlabel('Train Data Ratio')\n",
    "plt.ylabel('% of top-k scores found')\n",
    "plt.title('Comparison of Different Strategies')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "all_coords_df = pd.concat(all_coords)\n",
    "coords_save_path = os.path.join(args.save_dir, \"comparison_of_different_strategies_all_coordinates.csv\")\n",
    "all_coords_df.to_csv(coords_save_path, index=False)\n",
    "\n",
    "print(f\"All coordinates saved to: {coords_save_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
